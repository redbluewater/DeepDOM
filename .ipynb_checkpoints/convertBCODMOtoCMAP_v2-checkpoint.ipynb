{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9efb115-054a-4922-9312-dba2564fd872",
   "metadata": {
    "id": "a9efb115-054a-4922-9312-dba2564fd872"
   },
   "source": [
    "# convertBCODMOtoCMAP\n",
    "Krista Longnecker, 18 July 2025\\\n",
    "Run this after running ```getBCODMOinfo.ipynb```\\\n",
    "This script will make the format required by CMAP.\\\n",
    "Note: this version puts most of the code into separate python scripts (convert*.py) to make it easier to run through multiple data files.\\\n",
    "Also note that you end up running through this twice in order to get all the metadata needed for the _variables_. There are too many details needed by CMAP to automate that step.\\\n",
    "Editing to use this framework to get the DeepDOM data into CMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "45798c33-8867-439d-a1da-cbe323c0efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "6edb5289-064f-42ff-a807-68dcc169a7b6",
   "metadata": {
    "id": "6edb5289-064f-42ff-a807-68dcc169a7b6"
   },
   "outputs": [],
   "source": [
    "#some of these are residual from assembling the data file, keep for now.\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pdb\n",
    "\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from SPARQLWrapper import SPARQLWrapper, POST, JSON\n",
    "from frictionless import describe, Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b8e537cf-1082-4559-9320-12a8bddb03bb",
   "metadata": {
    "id": "b8e537cf-1082-4559-9320-12a8bddb03bb"
   },
   "outputs": [],
   "source": [
    "# Make a function that searches for bcodmo:name and returns bcodmo:description and bcodmo:units\n",
    "# input: md --> the list of parameters for one dataset\n",
    "def getDetails(md,bcodmo_name):\n",
    "    \"\"\"\n",
    "    Take the list of information from BCO-DMO, search for a name, and return the description and units for that name\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(md):\n",
    "        if item['bcodmo:name'] == bcodmo_name:\n",
    "            #actually want the descrption, so return that\n",
    "            description = item['bcodmo:description']\n",
    "            if 'bcodmo:units' in item:\n",
    "                units = item['bcodmo:units']\n",
    "            else:\n",
    "                units = 'not applicable'\n",
    "            #print(units)\n",
    "\n",
    "    return description, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "80d614f1-25a7-4738-9ef4-b442abcc6eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1e80f732-6ba1-48d9-a7e4-741cabbe836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Excel file with metadata\n"
     ]
    }
   ],
   "source": [
    "#It would be easier to make this file if does not exist...\n",
    "\n",
    "#Check that an Excel file exists for the variable metadata:\n",
    "if os.path.exists('CMAP_variableMetadata_additions.xlsx'):\n",
    "    print('Found Excel file with metadata')\n",
    "else:\n",
    "    #You cannot proceed without the file, so stop the script if it's not found\n",
    "    print(f\"No Excel file with metadata found\")\n",
    "    sys.exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7b33016a-3190-4863-964f-8a9976672387",
   "metadata": {
    "id": "7b33016a-3190-4863-964f-8a9976672387"
   },
   "outputs": [],
   "source": [
    "#read in the package that was already made (using getBCODMOinfo.ipynb)\n",
    "oneProject = Package('datapackage.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b2129d21-538d-4711-9138-fe776f1946f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_log',\n",
       " 'nutrients',\n",
       " 'ctd',\n",
       " 'viral_bact_counts',\n",
       " 'metabolites_dissolved',\n",
       " 'metabolites_particulate',\n",
       " 'tos',\n",
       " 'deepdom_sample_metadata_for_opp']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = [];\n",
    "for idx,item in enumerate(oneProject.resources):\n",
    "    justFile = item.name;\n",
    "    out.append(justFile)\n",
    "\n",
    "out #this will be a list, leave here so I see the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4ee7f62b-3cd0-4c4a-86f3-25fcdbea9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the event log first as I will need that for other samples (probably some at BCO-DMO and definitely the MTBLS1752 dataset at MetaboLights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "e74bd823-2c4e-47f2-aa12-3048a6e55fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_json = 0\n",
    "#to do: figure out a better way to do this so I am not reading in the json file every time\n",
    "biosscope = Package('datapackage.json')\n",
    "\n",
    "data_url = biosscope.resources[idx_json].path\n",
    "md = biosscope.resources[idx_json].custom['bcodmo:parameters'] #this is a list, don't forget 'custom' (!!)\n",
    "\n",
    "#make a short name out of the data_url, will use this as part of the name for the final Excel file \n",
    "exportFile = re.split('/',data_url).pop().replace('.csv','')\n",
    "\n",
    "#super easy to work with the CSV file once I have the URL\n",
    "eventLog = pd.read_csv(data_url,na_values = ['nd']) #now I have NaN...but they get dropped when writing the file\n",
    "    \n",
    "# Don't need to submit the event log to CMAP, just use it for merging to other datastreams.\n",
    "# need zfill to get a four digit time, otherwise this will fail, here the format is what the numbers are in (not what I want)\n",
    "eventLog['date_column'] = pd.to_datetime(eventLog['date_utc'].apply(str) + ' ' + eventLog['time_utc'].apply(str).str.zfill(4), format=\"%Y%m%d %H%M\")\n",
    "eventLog['time_cmap'] = eventLog['date_column'].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc677a32-10f4-4ece-9f0c-fd783d7493fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_json = int(sys.argv[1])\n",
    "idx_json = 1\n",
    "\n",
    "data_url = biosscope.resources[idx_json].path\n",
    "md = biosscope.resources[idx_json].custom['bcodmo:parameters'] #this is a list, don't forget 'custom' (!!)\n",
    "\n",
    "#make a short name out of the data_url, will use this as part of the name for the final Excel file \n",
    "exportFile = re.split('/',data_url).pop().replace('.csv','')\n",
    "\n",
    "#super easy to work with the CSV file once I have the URL\n",
    "bcodmo = pd.read_csv(data_url,na_values = ['nd']) #now I have NaN...but they get dropped when writing the file\n",
    "\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "\n",
    "# time --> CMAP requirement is this: #< Format  %Y-%m-%dT%H:%M:%S,  Time-Zone:  UTC,  example: 2014-02-28T14:25:55 >\n",
    "# Do this in two steps so I can check the output more easily\n",
    "temp = bcodmo.copy()\n",
    "temp['date'] = pd.to_datetime(temp['date_start_utc'].apply(str) + ' ' + temp['time_start_utc'].apply(str).str.zfill(4), format=\"%Y%m%d %H%M\")\n",
    "temp['date_cmap'] = temp['date'].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "df['time'] = temp['date_cmap']\n",
    "\n",
    "# lat (-90 to 90) and lon (-180 to 180); use variable names at BCO-DMO\n",
    "df['lat'] = bcodmo['lat_start']\n",
    "df['lon'] = bcodmo['lon_start']  #BCO-DMO already has this as negative\n",
    "df['depth'] = bcodmo['depth']\n",
    "\n",
    "# all remaining columns in bcodmo can be considered data\n",
    "#remember: bcodmo_trim will have the list of variables that I will use later to get metadata about the variables\n",
    "bcodmo_trim = bcodmo.drop(columns=['lat_start', 'lon_start', 'depth'])\n",
    "nVariables = bcodmo_trim.shape[1] #remember in Python indexing starts with 0 (rows, 1 is the columns)\n",
    "# and then add to the datafile I am assembling (essentially re-order columns\n",
    "df = pd.concat([df, bcodmo_trim], axis=1)\n",
    "   \n",
    "# work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "templateName = 'datasetTemplate.xlsx'\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(templateName, sheet_name=sheet_name)\n",
    "metaVarColumns = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadata about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=metaVarColumns,index = pd.RangeIndex(start=0,stop=nVariables)) #remember, Python is 0 indexed\n",
    "\n",
    "#the variables I need to search for are here: bcodmo_trim.columns, put them in the first column\n",
    "df2['var_short_name'] = bcodmo_trim.columns\n",
    "#Need the information from BCO-DMO to fill in the metadata about the variables.\n",
    "#md = biosscope.resources[idx].custom['bcodmo:parameters'] #this is a list, don't forget 'custom' (!!)\n",
    "    \n",
    "#there is most certainly a better way to do this, but I understand this option\n",
    "for idx,item in enumerate(df2.iterrows()):\n",
    "    a,b = getDetails(md,df2.loc[idx,'var_short_name']) #getDetails is the function I wrote (see above)\n",
    "    df2.loc[idx,'var_long_name'] = clean(a)\n",
    "    df2.loc[idx,'var_unit'] = b\n",
    "\n",
    "LUsensors = {'Alpkem RFA300':['NO3_NO2','silicate','NO2','PO4','NH4'],\n",
    "             'Shimadzu TOC-V':['NPOC','TN']\n",
    "                }\n",
    "\n",
    "#setup the column so Python does not make a column for floats\n",
    "df['var_sensor'] = \"\"\n",
    "\n",
    "# this will return the sensor given a possible variable, surely there is a better way to do this...\n",
    "for idx,item in enumerate(df2.iterrows()):\n",
    "    oneVar = df2.loc[idx,'var_short_name']\n",
    "    sensor =  str([k for k, v in LUsensors.items() if oneVar in v])[2:-2]\n",
    "    if len(sensor): #only try and fill in if a sensor was found\n",
    "        df2.loc[idx,'var_sensor'] = str(sensor)\n",
    "        \n",
    "#there are a few pieces of metadata that CMAP wants that will be easier to track in an Excel file. Right now this means I run through \n",
    "# this twice. Annoying, but haven't figured out a better way (yet).\n",
    "#The keywords include cruises, and all possible names for a variable.\n",
    "fName = 'CMAP_variableMetadata_additions.xlsx'\n",
    "sheetName = exportFile[0:31] #Excel limits the length of the sheet name\n",
    "moreMD = pd.read_excel(fName,sheet_name = sheetName)\n",
    "\n",
    "#suffixes are added to column name to keep them separate; '' adds nothing while '_td' adds _td that can get deleted next\n",
    "df2 = moreMD.merge(df2[['var_short_name','var_long_name','var_sensor','var_unit']],on='var_short_name',how='left',suffixes=('_td', '',))\n",
    "\n",
    "# Discard the columns that acquired a suffix:\n",
    "df2 = df2[[c for c in df2.columns if not c.endswith('_td')]]\n",
    "\n",
    "df2 = df2.loc[:,metaVarColumns]\n",
    "#these two are easy: just add them here\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "\n",
    "#metadata about the project    \n",
    "#stuck some variables in the Excel file and can pull them from here\n",
    "varProject = pd.read_excel(fName,sheet_name = 'project')\n",
    "one = varProject.loc[varProject['name'] == sheetName]\n",
    "\n",
    "# finally gather up the dataset_meta_data: for now I just wrote the information here, I might setup in a separate text file later\n",
    "#pdb.set_trace()\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': ['DeepDOM_v1'],\n",
    "    'dataset_long_name': ['DeepDOM ' + exportFile],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': [date.today()],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': [one.loc[:,'dataset_source'][0]],\n",
    "    'dataset_distributor': [one.loc[:,'dataset_distributor'][0]],\n",
    "    'dataset_acknowledgement': [one.loc[:,'dataset_acknowledgement'][0]],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': [biosscope.resources[idx_json].sources[0]['title']],\n",
    "    'dataset_references': [one.loc[:,'dataset_references'][0]],\n",
    "    'climatology': [0],\n",
    "    'cruise_names': 'KN210-04'\n",
    "    })\n",
    "\n",
    "\n",
    "#export the result as an Excel file with three tabs\n",
    "#make the data folder if it is not already there (it is in .gitignore, so it will not end up at GitHub)\n",
    "folder = \"data\"\n",
    "os.chdir(\".\")\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print(\"Data will go here: %s\" % (os.getcwd()) + '\\\\' + folder + '\\\\' + exportFile)\n",
    "else:\n",
    "    os.mkdir(folder)\n",
    "\n",
    "fName_CMAP = 'data/' + 'DeepDOM_BCODMO_' + exportFile + '.xlsx' \n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a31a5-90e2-477a-b7ed-6a26c5607aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fcdfaa2-c137-4c64-b851-9fc5fd2470d9",
   "metadata": {
    "id": "4fcdfaa2-c137-4c64-b851-9fc5fd2470d9"
   },
   "source": [
    "# Work though all of the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "460fcad7-34eb-4f5a-b1ae-64e42cdac7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most datasets need additional processing to get depth and/or time. \n",
    "#Set up a table here to note what needs to be done, use later to spin off to other scripts.\n",
    "#this essentially splits by PI since each PI has different project metadata\n",
    "toSkip = {'deepdom_sample_metadata_for_opp','event_log'}\n",
    "discreteData = {'nutrients','CTD','viral_bact_counts','metabolites_dissolved','metabolites_particulate','TOS'};\n",
    "\n",
    "#holding zone,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "df2291f9-c544-4475-b553-edaff47f8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip event_log\n",
      "Data will go here: C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\data\\nutrients\n",
      "no match CTD\n",
      "Data will go here: C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\data\\viral_bact_counts\n",
      "Data will go here: C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\data\\metabolites_dissolved\n",
      "Data will go here: C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\data\\metabolites_particulate\n",
      "no match TOS\n",
      "skip deepdom_sample_metadata_for_opp\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(oneProject.resources)):\n",
    "    data_url = oneProject.resources[idx].path\n",
    "    if data_url.endswith('.csv'):\n",
    "        checkFile = re.split('/',data_url).pop().replace('.csv','')\n",
    "        #have a few options and trying to group these based on added steps needed to make the data file ready\n",
    "        if checkFile in toSkip:\n",
    "            print('skip ' + checkFile)\n",
    "        # elif checkFile in pumpData:\n",
    "        #     %run convert_pumpData.py {idx}   \n",
    "        # elif checkFile in zoopData:\n",
    "        #     %run convert_zoopData.py {idx}   \n",
    "        elif checkFile in discreteData:\n",
    "            %run convert.py {idx}   \n",
    "        else:\n",
    "            print('no match ' + checkFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081b9d9-3e48-4803-bf72-d376c2b5c421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
