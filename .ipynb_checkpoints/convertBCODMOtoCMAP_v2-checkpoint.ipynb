{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9efb115-054a-4922-9312-dba2564fd872",
   "metadata": {
    "id": "a9efb115-054a-4922-9312-dba2564fd872"
   },
   "source": [
    "# convertBCODMOtoCMAP\n",
    "Krista Longnecker, 18 July 2025\\\n",
    "Run this after running ```getBCODMOinfo.ipynb```\\\n",
    "This script will make the format required by CMAP.\\\n",
    "Note: this version puts most of the code into separate python scripts (convert*.py) to make it easier to run through multiple data files.\\\n",
    "Also note that you end up running through this twice in order to get all the metadata needed for the _variables_. There are too many details needed by CMAP to automate that step.\\\n",
    "Editing to use this framework to get the DeepDOM data into CMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45798c33-8867-439d-a1da-cbe323c0efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edb5289-064f-42ff-a807-68dcc169a7b6",
   "metadata": {
    "id": "6edb5289-064f-42ff-a807-68dcc169a7b6"
   },
   "outputs": [],
   "source": [
    "#some of these are residual from assembling the data file, keep for now.\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pdb\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from SPARQLWrapper import SPARQLWrapper, POST, JSON\n",
    "from frictionless import describe, Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e537cf-1082-4559-9320-12a8bddb03bb",
   "metadata": {
    "id": "b8e537cf-1082-4559-9320-12a8bddb03bb"
   },
   "outputs": [],
   "source": [
    "# Make a function that searches for bcodmo:name and returns bcodmo:description and bcodmo:units\n",
    "# input: md --> the list of parameters for one dataset\n",
    "def getDetails(md,bcodmo_name):\n",
    "    \"\"\"\n",
    "    Take the list of information from BCO-DMO, search for a name, and return the description and units for that name\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(md):\n",
    "        if item['bcodmo:name'] == bcodmo_name:\n",
    "            #actually want the descrption, so return that\n",
    "            description = item['bcodmo:description']\n",
    "            if 'bcodmo:units' in item:\n",
    "                units = item['bcodmo:units']\n",
    "            else:\n",
    "                units = 'not applicable'\n",
    "            #print(units)\n",
    "\n",
    "    return description, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e80f732-6ba1-48d9-a7e4-741cabbe836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Excel file with metadata\n"
     ]
    }
   ],
   "source": [
    "#Check that an Excel file exists for the variable metadata:\n",
    "if os.path.exists('CMAP_variableMetadata_additions.xlsx'):\n",
    "    print('Found Excel file with metadata')\n",
    "else:\n",
    "    #You cannot proceed without the file, so stop the script if it's not found\n",
    "    print(f\"No Excel file with metadata found\")\n",
    "    sys.exit(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b33016a-3190-4863-964f-8a9976672387",
   "metadata": {
    "id": "7b33016a-3190-4863-964f-8a9976672387"
   },
   "outputs": [],
   "source": [
    "#read in the package that was already made (using getBCODMOinfo.ipynb)\n",
    "oneProject = Package('datapackage.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2129d21-538d-4711-9138-fe776f1946f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_log',\n",
       " 'nutrients',\n",
       " 'ctd',\n",
       " 'viral_bact_counts',\n",
       " 'metabolites_dissolved',\n",
       " 'metabolites_particulate',\n",
       " 'tos',\n",
       " 'deepdom_sample_metadata_for_opp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = [];\n",
    "for idx,item in enumerate(oneProject.resources):\n",
    "    justFile = item.name;\n",
    "    out.append(justFile)\n",
    "\n",
    "out #this will be a list, leave here so I see the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa6ab33-1bac-4e7f-8645-582ddf3feb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most datasets need additional processing to get depth and/or time. \n",
    "#Set up a table here to note what needs to be done, use later to spin off to other scripts.\n",
    "#this essentially splits by PI since each PI has different project metadata\n",
    "toSkip = {'deepdom_sample_metadata_for_opp'}\n",
    "discreteData = {'nutrients'};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcdfaa2-c137-4c64-b851-9fc5fd2470d9",
   "metadata": {
    "id": "4fcdfaa2-c137-4c64-b851-9fc5fd2470d9"
   },
   "source": [
    "# Work though all of the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedd6ec9-93f0-44ff-8a07-9e5ff0a0975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match event_log\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'decy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'decy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\convert.py:197\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#                                     #\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#                                     #\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m#                                     #\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m#######################################\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:    \n\u001b[1;32m--> 197\u001b[0m     main()\n",
      "File \u001b[1;32m~\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\convert.py:58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# time --> CMAP requirement is this: #< Format  %Y-%m-%dT%H:%M:%S,  Time-Zone:  UTC,  example: 2014-02-28T14:25:55 >\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Do this in two steps so I can check the output more easily\u001b[39;00m\n\u001b[0;32m     57\u001b[0m temp \u001b[38;5;241m=\u001b[39m bcodmo\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 58\u001b[0m temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecy\u001b[39m\u001b[38;5;124m'\u001b[39m], unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1970-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_cmap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_cmap\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'decy'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no match CTD\n",
      "no match viral_bact_counts\n",
      "no match metabolites_dissolved\n",
      "no match metabolites_particulate\n",
      "no match TOS\n",
      "skip deepdom_sample_metadata_for_opp\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(oneProject.resources)):\n",
    "    data_url = oneProject.resources[idx].path\n",
    "    if data_url.endswith('.csv'):\n",
    "        checkFile = re.split('/',data_url).pop().replace('.csv','')\n",
    "        #have a few options and trying to group these based on added steps needed to make the data file ready\n",
    "        if checkFile in toSkip:\n",
    "            print('skip ' + checkFile)\n",
    "        # elif checkFile in pumpData:\n",
    "        #     %run convert_pumpData.py {idx}   \n",
    "        # elif checkFile in zoopData:\n",
    "        #     %run convert_zoopData.py {idx}   \n",
    "        elif checkFile in discreteData:\n",
    "            %run convert.py {idx}   \n",
    "        else:\n",
    "            print('no match ' + checkFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bad143-1f83-4bf6-b94a-ab78b0d55290",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_json = 1\n",
    "oneProject = Package('datapackage.json')\n",
    "\n",
    "data_url = oneProject.resources[idx_json].path\n",
    "md = oneProject.resources[idx_json].custom['bcodmo:parameters'] #this is a list, don't forget 'custom' (!!)\n",
    "\n",
    "#make a short name out of the data_url, will use this as part of the name for the final Excel file \n",
    "exportFile = re.split('/',data_url).pop().replace('.csv','')\n",
    "\n",
    "#super easy to work with the CSV file once I have the URL\n",
    "bcodmo = pd.read_csv(data_url,na_values = ['nd']) #now I have NaN...but they get dropped when writing the file\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f5fb082-9880-491d-b6b5-afd5a74e36d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://datadocs.bco-dmo.org/file/WWW1EMYcyGpEYx/nutrients.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1db0589-cbba-43be-99df-ca771b59162e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cast</th>\n",
       "      <th>station</th>\n",
       "      <th>date_start_utc</th>\n",
       "      <th>time_start_utc</th>\n",
       "      <th>event_start</th>\n",
       "      <th>lat_start</th>\n",
       "      <th>lon_start</th>\n",
       "      <th>niskin</th>\n",
       "      <th>depth</th>\n",
       "      <th>press</th>\n",
       "      <th>bots</th>\n",
       "      <th>PO4</th>\n",
       "      <th>NO3_NO2</th>\n",
       "      <th>silicate</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NH4</th>\n",
       "      <th>NPOC</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20130327</td>\n",
       "      <td>1609</td>\n",
       "      <td>20130327.1609.001</td>\n",
       "      <td>-37.997297</td>\n",
       "      <td>-45.000042</td>\n",
       "      <td>10</td>\n",
       "      <td>5108.562</td>\n",
       "      <td>5209.693</td>\n",
       "      <td>10</td>\n",
       "      <td>2.4173</td>\n",
       "      <td>33.3321</td>\n",
       "      <td>126.0667</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>47.043420</td>\n",
       "      <td>37.323593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20130327</td>\n",
       "      <td>1609</td>\n",
       "      <td>20130327.1609.001</td>\n",
       "      <td>-37.997297</td>\n",
       "      <td>-45.000042</td>\n",
       "      <td>11</td>\n",
       "      <td>4001.029</td>\n",
       "      <td>4069.818</td>\n",
       "      <td>11</td>\n",
       "      <td>2.1939</td>\n",
       "      <td>30.1202</td>\n",
       "      <td>91.6798</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>46.527814</td>\n",
       "      <td>33.007255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20130327</td>\n",
       "      <td>1609</td>\n",
       "      <td>20130327.1609.001</td>\n",
       "      <td>-37.997297</td>\n",
       "      <td>-45.000042</td>\n",
       "      <td>12</td>\n",
       "      <td>3002.155</td>\n",
       "      <td>3046.643</td>\n",
       "      <td>12</td>\n",
       "      <td>1.9267</td>\n",
       "      <td>26.3843</td>\n",
       "      <td>60.5519</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>46.202492</td>\n",
       "      <td>28.239207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20130327</td>\n",
       "      <td>1609</td>\n",
       "      <td>20130327.1609.001</td>\n",
       "      <td>-37.997297</td>\n",
       "      <td>-45.000042</td>\n",
       "      <td>23</td>\n",
       "      <td>2501.794</td>\n",
       "      <td>2535.868</td>\n",
       "      <td>23</td>\n",
       "      <td>1.7849</td>\n",
       "      <td>25.5304</td>\n",
       "      <td>50.0437</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>44.600432</td>\n",
       "      <td>27.436167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20130328</td>\n",
       "      <td>1230</td>\n",
       "      <td>20130328.1230.001</td>\n",
       "      <td>-37.996300</td>\n",
       "      <td>-44.998833</td>\n",
       "      <td>1</td>\n",
       "      <td>503.511</td>\n",
       "      <td>507.930</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4213</td>\n",
       "      <td>19.4291</td>\n",
       "      <td>6.7561</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>51.217370</td>\n",
       "      <td>20.778466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cast  station  date_start_utc  time_start_utc        event_start  \\\n",
       "0     2        2        20130327            1609  20130327.1609.001   \n",
       "1     2        2        20130327            1609  20130327.1609.001   \n",
       "2     2        2        20130327            1609  20130327.1609.001   \n",
       "3     2        2        20130327            1609  20130327.1609.001   \n",
       "4     3        2        20130328            1230  20130328.1230.001   \n",
       "\n",
       "   lat_start  lon_start  niskin     depth     press  bots     PO4  NO3_NO2  \\\n",
       "0 -37.997297 -45.000042      10  5108.562  5209.693    10  2.4173  33.3321   \n",
       "1 -37.997297 -45.000042      11  4001.029  4069.818    11  2.1939  30.1202   \n",
       "2 -37.997297 -45.000042      12  3002.155  3046.643    12  1.9267  26.3843   \n",
       "3 -37.997297 -45.000042      23  2501.794  2535.868    23  1.7849  25.5304   \n",
       "4 -37.996300 -44.998833       1   503.511   507.930     1  1.4213  19.4291   \n",
       "\n",
       "   silicate     NO2     NH4       NPOC         TN  \n",
       "0  126.0667  0.0498  0.0210  47.043420  37.323593  \n",
       "1   91.6798  0.0406  0.0196  46.527814  33.007255  \n",
       "2   60.5519  0.0363  0.0283  46.202492  28.239207  \n",
       "3   50.0437  0.0271  0.0169  44.600432  27.436167  \n",
       "4    6.7561  0.0302  0.0315  51.217370  20.778466  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcodmo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3e041-7627-4d2b-aafc-a454c1d7a37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d091827-25c2-446d-968f-954ae5e12fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx_json = int(sys.argv[1])\n",
    "#to do: figure out a better way to do this so I am not reading in the json file every time\n",
    "oneProject = Package('datapackage.json')\n",
    "\n",
    "data_url = oneProject.resources[idx_json].path\n",
    "md = oneProject.resources[idx_json].custom['bcodmo:parameters'] #this is a list, don't forget 'custom' (!!)\n",
    "\n",
    "#make a short name out of the data_url, will use this as part of the name for the final Excel file \n",
    "exportFile = re.split('/',data_url).pop().replace('.csv','')\n",
    "\n",
    "#super easy to work with the CSV file once I have the URL\n",
    "bcodmo = pd.read_csv(data_url,na_values = ['nd']) #now I have NaN...but they get dropped when writing the file\n",
    "    \n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "\n",
    "# time --> CMAP requirement is this: #< Format  %Y-%m-%dT%H:%M:%S,  Time-Zone:  UTC,  example: 2014-02-28T14:25:55 >\n",
    "# Do this in two steps so I can check the output more easily\n",
    "temp = bcodmo.copy()\n",
    "temp['date'] = pd.to_datetime(temp['decy'], unit='D', origin='1970-01-01')\n",
    "temp['date_cmap'] = temp['date'].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "df['time'] = temp['date_cmap']\n",
    "\n",
    "# lat (-90 to 90) and lon (-180 to 180); use variable names at BCO-DMO\n",
    "df['lat'] = bcodmo['Latitude']\n",
    "df['lon'] = bcodmo['Longitude']  #BCO-DMO already has this as negative\n",
    "df['depth'] = bcodmo['Depth']\n",
    "\n",
    "# all remaining columns in bcodmo can be considered data\n",
    "#remember: bcodmo_trim will have the list of variables that I will use later to get metadata about the variables\n",
    "bcodmo_trim = bcodmo.drop(columns=['Latitude', 'Longitude', 'Depth'])\n",
    "nVariables = bcodmo_trim.shape[1] #remember in Python indexing starts with 0 (rows, 1 is the columns)\n",
    "# and then add to the datafile I am assembling (essentially re-order columns\n",
    "df = pd.concat([df, bcodmo_trim], axis=1)\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
