{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82afadc-b149-4bfd-9572-7febc8313c9e",
   "metadata": {},
   "source": [
    "# MetaboLights to CMAP\n",
    "## Krista Longnecker, 9 July 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7214-899f-4083-97fc-5d9bcc0cf63e",
   "metadata": {},
   "source": [
    "MetaboLights has FTP access to their data files and that is easy enough to access, but there are some downstream steps to add because I did not upload the full station information to MetaboLights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1fbb45-aad7-45aa-9535-7fde8cac1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5d5f61-73bd-432f-960a-976b2bcc70a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from ftplib import FTP\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from datetime import date\n",
    "\n",
    "#import libchebipy #pip install libChEBIpy\n",
    "from libchebipy._chebi_entity import ChebiEntity #this will get metabolite synonyms\n",
    "\n",
    "# import pdb\n",
    "#pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db13a103-9d08-4f8f-b39d-376694a712f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data will go here (but should not be synced to GitHub): C:\\Users\\klongnecker\\Documents\\Dropbox\\GitHub_espresso\\DeepDOM\\data\n"
     ]
    }
   ],
   "source": [
    "#make the data folder if it is not already there (it is in .gitignore, so it will not end up at GitHub)\n",
    "folder = \"data\"\n",
    "os.chdir(\".\")\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print(\"Data will go here (but should not be synced to GitHub): %s\" % (os.getcwd()) + '\\\\' + folder)\n",
    "else:\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff0279-52e0-4e2f-9fd1-81049aa61cdd",
   "metadata": {},
   "source": [
    "### Get MetaboLights files via FTP access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a7d19-9347-4dcd-88c3-e10fda32ca5c",
   "metadata": {},
   "source": [
    "The following cells read a file, write to disk, and then read the result into Python. I am not sure how to skip the middle/write step, but it seems like setting this up should work (but it doesn't)\\\n",
    "    # Create an in-memory binary stream\\\n",
    "    in_memory_file = io.BytesIO()\n",
    "\n",
    "Note: the frictionless website talks about FTP access to get data, but I cannot get it to work (by guessing, seems undocumented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3829600-5ab2-4c79-8bd8-bddd96b13cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with one dataset at MetaboLights, TSQ data described in Longnecker et al. 2024 (only other dataset ready is the untargeted data)\n",
    "study_id = 'MTBLS1752'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7d3689-ac17-40c8-8aad-b69625a2cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#while testing, if the FTP command fails the connection is left open and the next command gives error\n",
    "#error is: AttributeError: 'NoneType' object has no attribute 'sendall'\n",
    "ftp = FTP('ftp.ebi.ac.uk') #address from MetaboLights webpage\n",
    "ftp.login()\n",
    "ftpDataAddress = '/pub/databases/metabolights/studies/public/' + study_id\n",
    "ftp.cwd(ftpDataAddress)\n",
    "#ftp.retrlines('LIST') #this will only print to console, not what I want\n",
    "fileList = ftp.nlst() #can use this to make a list that will be searchable\n",
    "#fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1e7411-1324-4eca-bbe0-f396839be402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the metadata about the samples so I can convert each sample to time/lat/lon/depth to match the CMAP requirements\n",
    "str = 's_' + study_id #this is the search string for the data files\n",
    "metadataFiles = [v for v in fileList if str in v] \n",
    "metadataFiles = pd.DataFrame(metadataFiles,columns = ['files'])\n",
    "readFile = metadataFiles.loc[0,'files']\n",
    "\n",
    "# metadataFiles: put them here \n",
    "# Is there a way to download an FTP file and not write it disk?\n",
    "writeFile = 'data/' + 'tempMetadata.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "    except AttributeError as e: \n",
    "        print(f\"AttributeError during quit: {e} - connection was likely already closed.\")\n",
    "\n",
    "# now read in the result\n",
    "metadata_aboutSamples = pd.read_table(writeFile,delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0259b25e-a334-437b-a68d-1d74dead48d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the data files (more than one because things are split positive/negative ion mode...concatenate them later\n",
    "str = 'm_' + study_id #this is the search string for the data files\n",
    "dataFiles = [v for v in fileList if str in v] #Python syntax, will make a list\n",
    "dataFiles = pd.DataFrame(dataFiles,columns = ['files']) #I find the dataframe easier to manage than the list\n",
    "tsvFile = pd.DataFrame(); #this will be the data file\n",
    "\n",
    "# readDataFile = dataFiles.loc[0,'files']\n",
    "#have to do some concatenating here bc positive and negative ion mode data\n",
    "for idx in range(len(dataFiles)):\n",
    "    readDataFile = dataFiles.loc[idx,'files']\n",
    "    writeDataFile = 'data/' + 'tempData.tsv'\n",
    "          \n",
    "    with open(writeDataFile,'wb') as fp:\n",
    "        #try-except to make sure the FTP closes\n",
    "        try:\n",
    "            retr_command = f\"RETR {readDataFile}\"\n",
    "            #pdb.set_trace()\n",
    "            ftp.retrbinary(retr_command, fp.write)\n",
    "        except Exception as e: \n",
    "            print(f\"Error during quit: {e}\")\n",
    "    \n",
    "    #read in the temporary file and add to tsvFile file\n",
    "    tsvFile = pd.concat([tsvFile,pd.read_table(writeDataFile,delimiter = '\\t')],ignore_index=True) #append is no longer valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ec9562-a2d9-4647-845f-9ba58a378428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, details about the experiment are easy because the filename is generic\n",
    "#information about the project is in the i_Investigation file\n",
    "writeFile = 'data/' + 'i_Investigation.txt'\n",
    "readFile = 'i_Investigation.txt'\n",
    "\n",
    "with open(writeFile,'wb') as fp:\n",
    "    #try-except to make sure the FTP closes\n",
    "    try:\n",
    "        retr_command = f\"RETR {readFile}\"\n",
    "        ftp.retrbinary(retr_command, fp.write)\n",
    "    except Exception as e: \n",
    "        print(f\"Error during quit: {e}\")\n",
    "\n",
    "# del writeFile, readFile\n",
    "\n",
    "#open up the txt file with the experiment data\n",
    "writeFile = 'data/i_Investigation.txt'  \n",
    "with open(writeFile, 'r') as f:\n",
    "    metadata_aboutExperiment = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59aba8d-69c4-437a-a6eb-bd87abfc7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this might not be necessary, but leave here in case I go back to it.\n",
    "# files = {}\n",
    "# files['data'] = tsvFile #this also holds the metadata_variables\n",
    "# files['metadata_project'] = metadata_aboutExperiment\n",
    "# files['metadata_samples'] = metadata_aboutSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46331992-041c-4e3a-b5e4-f7b447a5bad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp.quit()  #close the FTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f432b6f-7f0c-4087-b78d-d17df6d15bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source Name</th>\n",
       "      <th>Characteristics[Organism]</th>\n",
       "      <th>Term Source REF</th>\n",
       "      <th>Term Accession Number</th>\n",
       "      <th>Characteristics[Organism part]</th>\n",
       "      <th>Term Source REF.1</th>\n",
       "      <th>Term Accession Number.1</th>\n",
       "      <th>Characteristics[Variant]</th>\n",
       "      <th>Term Source REF.2</th>\n",
       "      <th>Term Accession Number.2</th>\n",
       "      <th>...</th>\n",
       "      <th>Unit.1</th>\n",
       "      <th>Term Source REF.8</th>\n",
       "      <th>Term Accession Number.8</th>\n",
       "      <th>Factor Value[Longitude]</th>\n",
       "      <th>Unit.2</th>\n",
       "      <th>Term Source REF.9</th>\n",
       "      <th>Term Accession Number.9</th>\n",
       "      <th>Factor Value[Sample group]</th>\n",
       "      <th>Term Source REF.10</th>\n",
       "      <th>Term Accession Number.10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KO1</td>\n",
       "      <td>sea water</td>\n",
       "      <td>ENVO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/ENVO_00002149</td>\n",
       "      <td>endometabolome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>0.2 µm Omnipore filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KO2</td>\n",
       "      <td>sea water</td>\n",
       "      <td>ENVO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/ENVO_00002149</td>\n",
       "      <td>endometabolome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>0.2 µm Omnipore filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KO3</td>\n",
       "      <td>sea water</td>\n",
       "      <td>ENVO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/ENVO_00002149</td>\n",
       "      <td>endometabolome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>0.2 µm Omnipore filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KO4</td>\n",
       "      <td>sea water</td>\n",
       "      <td>ENVO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/ENVO_00002149</td>\n",
       "      <td>endometabolome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>0.2 µm Omnipore filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KO5</td>\n",
       "      <td>sea water</td>\n",
       "      <td>ENVO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/ENVO_00002149</td>\n",
       "      <td>endometabolome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>degree</td>\n",
       "      <td>UO</td>\n",
       "      <td>http://purl.obolibrary.org/obo/UO_0000185</td>\n",
       "      <td>0.2 µm Omnipore filter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source Name Characteristics[Organism] Term Source REF  \\\n",
       "0         KO1                 sea water            ENVO   \n",
       "1         KO2                 sea water            ENVO   \n",
       "2         KO3                 sea water            ENVO   \n",
       "3         KO4                 sea water            ENVO   \n",
       "4         KO5                 sea water            ENVO   \n",
       "\n",
       "                          Term Accession Number  \\\n",
       "0  http://purl.obolibrary.org/obo/ENVO_00002149   \n",
       "1  http://purl.obolibrary.org/obo/ENVO_00002149   \n",
       "2  http://purl.obolibrary.org/obo/ENVO_00002149   \n",
       "3  http://purl.obolibrary.org/obo/ENVO_00002149   \n",
       "4  http://purl.obolibrary.org/obo/ENVO_00002149   \n",
       "\n",
       "  Characteristics[Organism part]  Term Source REF.1  Term Accession Number.1  \\\n",
       "0                 endometabolome                NaN                      NaN   \n",
       "1                 endometabolome                NaN                      NaN   \n",
       "2                 endometabolome                NaN                      NaN   \n",
       "3                 endometabolome                NaN                      NaN   \n",
       "4                 endometabolome                NaN                      NaN   \n",
       "\n",
       "   Characteristics[Variant]  Term Source REF.2  Term Accession Number.2  ...  \\\n",
       "0                       NaN                NaN                      NaN  ...   \n",
       "1                       NaN                NaN                      NaN  ...   \n",
       "2                       NaN                NaN                      NaN  ...   \n",
       "3                       NaN                NaN                      NaN  ...   \n",
       "4                       NaN                NaN                      NaN  ...   \n",
       "\n",
       "   Unit.1 Term Source REF.8                    Term Accession Number.8  \\\n",
       "0  degree                UO  http://purl.obolibrary.org/obo/UO_0000185   \n",
       "1  degree                UO  http://purl.obolibrary.org/obo/UO_0000185   \n",
       "2  degree                UO  http://purl.obolibrary.org/obo/UO_0000185   \n",
       "3  degree                UO  http://purl.obolibrary.org/obo/UO_0000185   \n",
       "4  degree                UO  http://purl.obolibrary.org/obo/UO_0000185   \n",
       "\n",
       "  Factor Value[Longitude]  Unit.2 Term Source REF.9  \\\n",
       "0                   -45.0  degree                UO   \n",
       "1                   -45.0  degree                UO   \n",
       "2                   -45.0  degree                UO   \n",
       "3                   -45.0  degree                UO   \n",
       "4                   -45.0  degree                UO   \n",
       "\n",
       "                     Term Accession Number.9  Factor Value[Sample group]  \\\n",
       "0  http://purl.obolibrary.org/obo/UO_0000185      0.2 µm Omnipore filter   \n",
       "1  http://purl.obolibrary.org/obo/UO_0000185      0.2 µm Omnipore filter   \n",
       "2  http://purl.obolibrary.org/obo/UO_0000185      0.2 µm Omnipore filter   \n",
       "3  http://purl.obolibrary.org/obo/UO_0000185      0.2 µm Omnipore filter   \n",
       "4  http://purl.obolibrary.org/obo/UO_0000185      0.2 µm Omnipore filter   \n",
       "\n",
       "   Term Source REF.10  Term Accession Number.10  \n",
       "0                 NaN                       NaN  \n",
       "1                 NaN                       NaN  \n",
       "2                 NaN                       NaN  \n",
       "3                 NaN                       NaN  \n",
       "4                 NaN                       NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_aboutSamples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afc28d-ed27-4957-9875-71d3a125015d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b8105-0b2a-4200-9cd9-f54b26cd8383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522aeca3-1a61-4685-bcfe-5726db85be2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Factor Value[Sampling year date]', 'Factor Value[Sampling month date]',\\n       'Factor Value[Sampling day date]', 'Factor Value[Hour of the day]',\\n       'Factor Value[Minute of the hour]'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m depth \u001b[38;5;241m=\u001b[39m metadata_aboutSamples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor Value[Depth]\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m temp \u001b[38;5;241m=\u001b[39m metadata_aboutSamples[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor Value[Sampling year date]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor Value[Sampling month date]\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor Value[Sampling day date]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor Value[Hour of the day]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFactor Value[Minute of the hour]\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      8\u001b[0m temp\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminute\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m step1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;28mdict\u001b[39m(year\u001b[38;5;241m=\u001b[39mtemp\u001b[38;5;241m.\u001b[39myear,month\u001b[38;5;241m=\u001b[39mtemp\u001b[38;5;241m.\u001b[39mmonth,day \u001b[38;5;241m=\u001b[39m temp\u001b[38;5;241m.\u001b[39mday,hour \u001b[38;5;241m=\u001b[39m temp\u001b[38;5;241m.\u001b[39mhour,minute\u001b[38;5;241m=\u001b[39mtemp\u001b[38;5;241m.\u001b[39mminute))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['Factor Value[Sampling year date]', 'Factor Value[Sampling month date]',\\n       'Factor Value[Sampling day date]', 'Factor Value[Hour of the day]',\\n       'Factor Value[Minute of the hour]'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# pull what I can from the sample information at MetaboLights\n",
    "#pull 'Source Name' as I need that later to match to columns in the data file\n",
    "sampleNames  = metadata_aboutSamples['Source Name']\n",
    "depth = metadata_aboutSamples['Factor Value[Depth]']\n",
    "\n",
    "#have station/cast/depth/lat/lon...but no time information in here\n",
    "\n",
    "#time is messier and the MetaboLights columns names are long, so shorten them to make this easier\n",
    "temp = metadata_aboutSamples[['Factor Value[Sampling year date]','Factor Value[Sampling month date]',\n",
    "                 'Factor Value[Sampling day date]','Factor Value[Hour of the day]','Factor Value[Minute of the hour]']]\n",
    "temp.columns = ['year','month','day','hour','minute']\n",
    "\n",
    "step1 = pd.to_datetime(dict(year=temp.year,month=temp.month,day = temp.day,hour = temp.hour,minute=temp.minute))\n",
    "date_cmap = step1.dt.strftime(\"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582611af-9c91-4fb4-bfc2-fb3ff31b4ecd",
   "metadata": {},
   "source": [
    "#### Need DeepDOM details from BCO-DMO to get time information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494987b-2dd1-406c-8481-24226176484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need the BIOS-SCOPE discrete data file for station information - that will have both BATS and BIOS-SCOPE data in it\n",
    "#this is slow, so leave as its own cell\n",
    "fName = 'data/XXX.xlsx';\n",
    "BSdata = pd.read_excel(open(fName,'rb'),sheet_name = 'DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9bb7f3-8356-4d6f-8450-b8946c4ac59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then setup the ability to use the information in BSdata by mathing sample names:\n",
    "\n",
    "#MetaboLights required samples to begin with a letter, I used 's' and need to strip that out \n",
    "NewID_inMTBLS  = pd.to_numeric(sampleNames.str.strip('s')) \n",
    "#convert the series into a dataframe:\n",
    "s_df = NewID_inMTBLS.reset_index()\n",
    "\n",
    "#use merge as it will be sorted in the right order; only keep rows with data found in MetaboLights\n",
    "merged_df = pd.merge(BSdata,s_df,how='right',left_on='New_ID',right_on='Source Name')\n",
    "\n",
    "#get the list of cruise names and find the unique ones - this will go on the sheet with metadata about the project\n",
    "unCruises = pd.DataFrame(merged_df.loc[:,'Cruise_ID'].unique())\n",
    "unCruises.columns = ['cruise_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2610669-38c2-4bb5-9474-e220179dfbfc",
   "metadata": {},
   "source": [
    "### Operate on the files I collected via FTP\n",
    "Remember data are in the tsvFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c8a99-440c-4046-a73d-57ca39e29bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvFile.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0f016-d660-43e0-847b-3d746af83aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#column with metabolite name is  (database identifier would be more generic, need to talk to CMAP people about this)\n",
    "mtabColumn = 'database_identifier'\n",
    "# mtabColumn = 'metabolite_identification' \n",
    "\n",
    "#only keep the columns that are in sampleNames\n",
    "dataColumns = tsvFile.columns[tsvFile.columns.isin(sampleNames)]\n",
    "dataOnly = tsvFile.loc[:,dataColumns].transpose() #index is the 's' numbered samples\n",
    "\n",
    "dataOnly.columns = tsvFile[mtabColumn] #label the columns with the metabolite information, will also use this for the sheet with metadata about the variables\n",
    "nVariables = len(dataOnly.columns) #need this for the sheet for the metadata on the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132079fe-c381-48fd-b638-cfbd92ae5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assemble the data into the CMAP format\n",
    "'''\n",
    "# Required variables are time, lat, lon, depth\n",
    "df = pd.DataFrame(columns=['time','lat','lon','depth'])\n",
    "df['time'] = date_cmap.to_frame()\n",
    "df['depth'] = depth.to_frame()\n",
    "df['lat'] = merged_df['latN'].to_frame()\n",
    "df['lon'] = -merged_df['lonW'].to_frame() #need negative number to put this into -180 to 180 space\n",
    "#df.insert(1,'test',merged_df['New_ID']) #check that I have the indexing right\n",
    "#df.insert(1,'test2',s_df['Source Name'])\n",
    "df.insert(1,'forIndex',sampleNames) #need an index to keep the rows matched up\n",
    "df.set_index('forIndex',inplace=True)\n",
    "\n",
    "#concatenate with the data in dataOnly\n",
    "df = pd.concat([df,dataOnly],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fb73e-a634-461b-b0dc-fafce7e199bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Work on the second sheet: metadata about the variables; use the CMAP dataset template to setup the dataframe so I get the column headers right\n",
    "'''\n",
    "fName = 'datasetTemplate.xlsx' #downloaded from CMAP website\n",
    "sheet_name = 'vars_meta_data'\n",
    "vars = pd.read_excel(fName, sheet_name=sheet_name)\n",
    "cols = vars.columns.tolist()\n",
    "#df2 will be the dataframe with the metadata about the variables, set it up empty here\n",
    "df2 = pd.DataFrame(columns=cols,index = pd.RangeIndex(0,nVariables,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb35bc3-5bb8-4888-bd4c-bc3bd6968ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Define a function to use the the chebi tool to find synonyms. Krista Longnecker, 12 July 2025\n",
    "    input: oneName is a string with the CHEBI number (just the number, no prefix)\n",
    "'''\n",
    "def getSynonym(oneName):\n",
    "    possibles = ChebiEntity(oneName).get_names()\n",
    "    justNames = []\n",
    "    for idx in range(len(possibles)):\n",
    "        justNames.append(possibles[idx].get_name())\n",
    "    \n",
    "    return justNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need details (metadata) about the metabolites \n",
    "\n",
    "# this is only a partial list of variables for the moment\n",
    "df2['var_short_name'] = dataOnly.columns\n",
    "df2.loc[:,'var_long_name'] = tsvFile.loc[:,'metabolite_identification']\n",
    "df2.loc[:,'var_sensor'] = 'Triple quadrupole mass spectrometer (TSQ Vantage, Thermo Scientific)'\n",
    "df2.loc[:,'var_unit'] = 'pM' #this is in the protocols, but I also have some inside information here\n",
    "df2.loc[:,('var_spatial_res')] = 'irregular'\n",
    "df2.loc[:, ('var_temporal_res')] = 'irregular'\n",
    "df2.loc[:,('var_discipline')] = 'chemistry'\n",
    "df2.loc[:,('visualize')] = 1 #yes/no, all metabolites can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751b6a3-63e0-425b-ac85-194e6fc36ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use unCruises to get the list of cruises, put that at the beginning, then add text that will work for the whole project\n",
    "misc_keywords = ','.join(unCruises['cruise_names']) + ', Woods Hole Oceanographic Institution, bio, biogeo, biogeochemistry, biology, BIOS, Bottle, cruise, Discrete, in situ, insitu, in-situ, North Atlantic Ocean, observation, '\n",
    "#then need to add the string with the names for each metabolite...do that in the next loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5a946-cca7-453e-9ebe-284d7f87f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through one metabolite at a time and make var_keywords\n",
    "#'var_keywords' will be the hardest as the metabolites have many, many keywords. \n",
    "for idx,item in df2.iterrows():\n",
    "    justNames = ', '.join(getSynonym(df2.loc[idx,'var_short_name'].strip('CHEBI:')))\n",
    "    df2.loc[idx,'var_keywords'] = misc_keywords + justNames\n",
    "    #print(df2.loc[idx,'var_short_name'])\n",
    "    del justNames    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d026a27-3531-4447-8546-3d5fbf007406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function to pull details from MetaboLights i file\n",
    "#this goes through one at a time...could alter to get all in one function (get stuff, and clean stuff as two steps)\n",
    "def getMetaboLights(pattern,text):\n",
    "    #pattern = r'Study Identifier(.*)' # Captures anything after \"Study Identifier = \"\n",
    "    extracted_data = re.findall(pattern, text) #this is a list\n",
    "    extracted_data = ' '.join(extracted_data) # ? really, this seems odd, but works.\n",
    "    \n",
    "    #tidy up the string\n",
    "    original_string = extracted_data\n",
    "    chars_to_remove = ['<p>', '</p>','\\t']\n",
    "    \n",
    "    # Using regular expressions (for more complex patterns or multiple occurrences)\n",
    "    pattern = \"|\".join(map(re.escape, chars_to_remove)) # Escapes special characters for regex\n",
    "    oneVar = re.sub(pattern, \"\", original_string)\n",
    "    return oneVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48d69a-0148-4d5f-ba15-7771ac2f89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gather the details I need from the metadata_aboutExperiment\n",
    "'''\n",
    "project_description = getMetaboLights(r'Study Description(.*)',metadata_aboutExperiment)\n",
    "dataset_short_name = getMetaboLights(r'Study Identifier(.*)',metadata_aboutExperiment)\n",
    "dataset_long_name = getMetaboLights(r'Study Title(.*)',metadata_aboutExperiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77721875-58ef-4cd1-97b0-a4a992877e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather up the dataset_meta_data into df3\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'dataset_short_name': [dataset_short_name],\n",
    "    'dataset_long_name': [dataset_long_name],\n",
    "    'dataset_version': ['1.0'],\n",
    "    'dataset_release_date': [date.today()],\n",
    "    'dataset_make': ['observation'],\n",
    "    'dataset_source': ['Elizabeth Kujawinski, Woods Hole Oceanographic Institution'],\n",
    "    'dataset_distributor': ['Elizabeth Kujawinski, Woods Hole Oceanographic Institution'],\n",
    "    'dataset_acknowledgement': ['We thank the captain, crew, and marine technicians of the RV Knorr for assistance during the cruise. We thank the DeepDOM team for their efforts during the cruise, and the work of many people and labs for assistance in the years that followed the cruise. This cruise supported by a grant from the National Science Foundation.'],\n",
    "    'dataset_history': [''],\n",
    "    'dataset_description': [project_description],\n",
    "    'dataset_references': ['https://www.ebi.ac.uk/metabolights/editor/' + study_id],\n",
    "    'climatology': [0]\n",
    "    })\n",
    "\n",
    "\n",
    "#insert the list of unique cruises from the data pulled from the BIOS-SCOPE discrete file\n",
    "df3 = pd.concat([df3,unCruises],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90278f7b-ee3f-411e-b195-42cadbf4e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "fName_CMAP = 'data/' + 'DeepDOM_MetaboLights_' + study_id + '.xlsx'\n",
    "dataset_names = {'data': df, 'dataset_meta_data': df3, 'vars_meta_data': df2}\n",
    "with pd.ExcelWriter(fName_CMAP) as writer:\n",
    "    for sheet_name, data in dataset_names.items():\n",
    "        data.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf5fd0-ae4c-4577-ae83-88be8930465f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b3784-ff42-4341-be12-ee3e3d4e10c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c2e7d-def5-4434-9348-f45e4d24e1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2edcd-e999-472c-9086-3809606278e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
